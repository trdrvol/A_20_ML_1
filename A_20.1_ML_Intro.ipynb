{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. What are the three stages to build the hypotheses or model in machine learning?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Three stages of model building in ML are :\n",
    "    a) Model building\n",
    "    b) Model testing\n",
    "    c) Application of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. What is the standard approach to supervised learning?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "After the data is preprocessed and cleaned the standard approach is to split the data into \n",
    "training (preferably training + cross validation) and test set. The model is trained on training set, cross validated\n",
    "and then classifications/predictions are done on test set. Accuracies are checked with different error metrics.\n",
    "If prediction/classification accuracy is not great then we can play around with different parameters,hyperparameters or \n",
    "consider various ensemble techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. What is Training set and Test set?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The whole population(of data) is divided into two distinct and separate sets.\n",
    "Generally the ratio is 80/20 (training/test set).\n",
    "If cross-validation is adopted in training set then ratio can be 60/20/20 (training/cross-validation/test set).\n",
    "Training set is examples/instances given to the learner/ML algo to fit and test set is used to test the accuracy \n",
    "of the hypothesis generated by the learner.Training set are distinct from test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4. What is the general principle of an ensemble method and what is bagging and\n",
    "boosting in ensemble method?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As the name suggest ensemble method does nothing but averages output of bunch of algorithms\n",
    "either as a normal average (bagging) or sequential weighted averages(boosting) in order to classify/predict better than \n",
    "a single classifier/predictor. \n",
    "There are few other ensemble methods but bagging and boosting are two most popular ensemble methods.\n",
    "Bagging,eg. Random forest, tackles Variance by averaging bunch of learners and Boosting(Adaboost,Gradient and XGBoost)\n",
    "tackles bias by sequentially giving different weights to instances.\n",
    "A bias term measures how closely the average classifier produced by the learning algorithm matches the target function. \n",
    "The variance term measures how much the learning algorithmâ€™s prediction fluctuates for different training sets.\n",
    "Boosting starts with equal weights given to instances in first weak learner.\n",
    "The wrongly predicted instances are given more weight and next learner gives more\n",
    "attention to the wrongly identified examples.\n",
    "This process goes on depending upon number of classifiers/regressors we have choosen\n",
    "the output of which satisfactorially falls in our accuracy bracket.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. How can you avoid overfitting ?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Overfitting by Ml model happens when the model describes random error or noise rather than the underlying relationship.\n",
    "Overfitting happens when the learner learns the training data set too well and the predictions accuracy is \n",
    "extremely high on training set but when learner is introduced to test set then prediction/classification accuracy nowhere\n",
    "near the training set accuracy. \n",
    "Overfitting is more likely with nonparametric and nonlinear models that have more flexibility when learning a target function\n",
    "Overfitting can be avoided by following methods:\n",
    "a) increase the data size.\n",
    "b) cross-validation in training set should also be used to counter overfitting.\n",
    "c) if data is small then make sure that number of features are far less than number of instances.\n",
    "d) to control features regularization parameter is introduced in cost function. \n",
    "  Excessively complex models with polynomial parameters tends to overfit and hence regularization term(lambda) should be introduced.\n",
    "  From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters. \n",
    "  Regularization can be used to learn simpler models, induce models to be sparse, introduce group structure into the learning problem. \n",
    "  The advantage of imposing priors for instance through regularisation is that the parameters are either shrunk to zero or some other\n",
    "  predefined value, and thus you are implicitly constraining the parameters and reducing the \"freedom\" of your model to overfit.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
